id,question,choices,answerKey
lecture_slide_02_topic,"What is the primary topic of Lecture 02, according to the provided slide title?","{""text"": [""Tuning Techniques for Cost Optimization"", ""An introduction to Innopolis University's faculty"", ""Hamza Salem's biography and research interests"", ""Advanced algorithms for data processing""], ""label"": [""A"", ""B"", ""C"", ""D""]}",A
lecture1_2,"According to the provided agenda, which topic will be discussed first?","{""text"": [""Fine-Tuning and Customizability"", ""Parameter-Efficient Fine-Tuning Methods"", ""Cost and Performance Trade-Offs"", ""Summary and Q&A""], ""label"": [""A"", ""B"", ""C"", ""D""]}",A
lecture1_3,"According to the slide, what is the primary purpose of fine-tuning a pre-trained Large Language Model (LLM)?","{""text"": [""To adjust a pre-trained LLM on a specific dataset to improve performance on a particular task."", ""To create a new LLM from scratch without using any existing models."", ""To make the LLM's general performance worse for the sake of extreme specialization."", ""To exclusively use the LLM for general, non-specific tasks without any customization.""], ""label"": [""A"", ""B"", ""C"", ""D""]}",A
lecture1_5,Which parameter-efficient fine-tuning technique involves compressing a large model (teacher) into a smaller model (student) while retaining most of its performance?,"{""text"": [""Distillation"", ""Pruning"", ""Quantization"", ""Feature Engineering""], ""label"": [""A"", ""B"", ""C"", ""D""]}",A
lecture1_6,"According to the lecture slide on Distillation, what is the primary benefit of deploying the Student Model in production?","{""text"": [""It enhances the complexity and size of the Teacher Model."", ""It acts as a guide to help train the initial Teacher Model."", ""It allows for significant cost savings due to its smaller size."", ""It provides more diverse and varied outputs than the Teacher Model.""], ""label"": [""A"", ""B"", ""C"", ""D""]}",C
lecture1_7,What is the primary reason for deploying the Student Model after the knowledge distillation process?,"{""text"": [""To provide guidance to the Teacher Model."", ""To serve as a larger, more complex model than the Teacher."", ""To achieve cost savings in production."", ""To train the Teacher Model on new predictions.""], ""label"": [""A"", ""B"", ""C"", ""D""]}",C
lecture1_8,"According to the slide, what is the primary purpose of 'pruning' in model optimization?","{""text"": [""Trimming the model by removing redundant parameters, such as nodes that contribute minimally to the output."", ""Lowering the precision of weights from 32-bit floats to 8-bit integers to reduce memory usage."", ""Sacrificing model accuracy for faster processing and less storage."", ""Enabling model deployment on resource-constrained devices like mobile phones.""], ""label"": [""A"", ""B"", ""C"", ""D""]}",A
lecture1_9,"According to the lecture slide on ""Cost and Performance Implications,"" what is a key consideration when deciding to use smaller, more parameter-efficient models?","{""text"": [""Reducing model size for efficiency often leads to a trade-off where accuracy or overall performance may be affected."", ""Smaller models are inherently better performers on complex tasks due to their streamlined architecture."", ""Scalability is primarily improved by increasing model size, not reducing it."", ""Parameter-efficient models are exclusively designed for complex decision-making processes.""], ""label"": [""A"", ""B"", ""C"", ""D""]}",A
lecture1_10,"According to the key takeaways, what is crucial for the successful deployment of any AI model optimization technique?","{""text"": [""Understanding the inherent pros and cons (trade-offs) of each technique."", ""Ensuring the highest possible customizability without any performance loss."", ""Always prioritizing cost optimization above all other factors, including performance."", ""Focusing solely on Parameter-Efficient Methods like distillation and pruning for all optimizations.""], ""label"": [""A"", ""B"", ""C"", ""D""]}",A
lecture1_11,What is the primary purpose of the links provided on this slide?,"{""text"": [""To provide access to practical demonstrations and code examples related to the lecture topic."", ""To serve as primary reference materials for theoretical concepts discussed in the lecture."", ""To link to external resources for academic research papers and literature reviews."", ""To guide users to the course's official website for administrative information.""], ""label"": [""A"", ""B"", ""C"", ""D""]}",A
lecture1_12,What is the primary objective of Exercise 1 regarding data preparation?,"{""text"": [""To use Google Gemini to clean and translate raw data into a structured JSON format for chat messages."", ""To develop a Python script that trains a chatbot named Marv using raw data."", ""To submit a `data.txt` file containing untranslated and uncleaned information."", ""To generate a CSV file from existing data using Google Gemini.""], ""label"": [""A"", ""B"", ""C"", ""D""]}",A
lecture1_13,"According to the lecture slide for 'Exercise 2: Fine-Tuning GPT', what is a key deliverable or demonstration required from the submitted Python code?","{""text"": [""The code should illustrate how the fine-tuned GPT2 model answers a question similar to those present in the dataset."", ""The code should generate a completely new dataset to be used for GPT2 fine-tuning."", ""The code should implement a GPT2 model from scratch without using any pre-trained weights."", ""The code should compare the performance of GPT2 with other generative pre-trained transformer models.""], ""label"": [""A"", ""B"", ""C"", ""D""]}",A
lecture1_14,What is the final step required to complete and submit the YTubeGPT optional exercise?,"{""text"": [""Send the deployed Telegram bot, which utilizes the prepared YouTube channel transcripts, to @enghamzasalem."", ""Prepare a dataset of YouTube video transcripts for tuning either a GPT2 or GPT4 model."", ""Choose a YouTube channel and successfully extract all of its video transcripts."", ""Deploy a fine-tuned GPT2 or GPT4 model based on the YouTube channel transcripts.""], ""label"": [""A"", ""B"", ""C"", ""D""]}",A
lecture1_15,The resources listed on this slide primarily offer guidance and information on which of the following topics?,"{""text"": [""Strategies for fine-tuning Large Language Models (LLMs) and prompt engineering."", ""The latest advancements in quantum computing hardware."", ""Fundamentals of traditional machine learning algorithms like decision trees and SVMs."", ""Web development frameworks and database management.""], ""label"": [""A"", ""B"", ""C"", ""D""]}",A
lecture2_1,What is the main subject covered in this lecture?,"{""text"": [""Inference Techniques for Cost Optimization"", ""The history of Innopolis University"", ""Biographical information about Hamza Salem"", ""General strategies for university administration""], ""label"": [""A"", ""B"", ""C"", ""D""]}",A
lecture2_2,"According to the provided agenda, which of the following topics will NOT be covered during the lecture?","{""text"": [""Advanced Inference Techniques"", ""Retrieval Augmented Generation"", ""Model Quantization Strategies"", ""Batch Prompting Strategies""], ""label"": [""A"", ""B"", ""C"", ""D""]}",C
lecture2_4,"What does the term ""inference"" primarily refer to when discussing Large Language Models (LLMs)?","{""text"": [""The process of using a trained model to generate predictions or outputs based on new input data."", ""The method of training a model on vast datasets to learn patterns."", ""The technique for evaluating the model's internal architecture and parameters."", ""The act of collecting and preparing new input data for future model development.""], ""label"": [""A"", ""B"", ""C"", ""D""]}",A
lecture2_5,"Which technique for efficient inference is illustrated by the example: 'storing Thor’s previous battles in the Avengers database, so when a similar threat arises, you don’t need to re-strategize from scratch'?","{""text"": [""Caching with Vector Stores"", ""Prompt Engineering"", ""Model Fine-tuning"", ""Ensemble Learning""], ""label"": [""A"", ""B"", ""C"", ""D""]}",A
lecture2_6,"According to the slide, what are two solutions proposed for handling long documents with LLMs?","{""text"": [""Chunking and Summarization"", ""Fine-tuning the LLM and increasing model size"", ""Reducing model parameters and increasing inference speed"", ""Expanding document context and utilizing larger datasets""], ""label"": [""A"", ""B"", ""C"", ""D""]}",A
lecture2_7,"According to the lecture slide, which of the following is a described strategy to mitigate the challenge of long documents overwhelming Large Language Models (LLMs)?","{""text"": [""Breaking down long documents into smaller, manageable sections."", ""Requiring LLMs to process the entire document at once, regardless of length."", ""Fine-tuning the LLM on a larger, more diverse dataset."", ""Limiting the LLM's response length to avoid high inference costs.""], ""label"": [""A"", ""B"", ""C"", ""D""]}",A
lecture2_8,"According to the lecture slide, which of the following is a primary benefit of implementing Batch Prompting Strategies?","{""text"": [""Reduced latency due to faster processing."", ""Enhanced model interpretability and debugging."", ""Greater flexibility for diverse and unrelated queries."", ""Increased computational expenses per prompt.""], ""label"": [""A"", ""B"", ""C"", ""D""]}",A
lecture2_9,"According to the lecture slide, what is identified as a critical benefit of employing inference techniques in real-time and large-scale applications?","{""text"": [""Managing costs effectively"", ""Improving data security protocols"", ""Streamlining user authentication"", ""Accelerating software deployment cycles""], ""label"": [""A"", ""B"", ""C"", ""D""]}",A
lecture2_10,What is Retrieval Augmented Generation (RAG)?,"{""text"": [""A method that combines a retrieval component with a language model to generate responses grounded in specific retrieved information."", ""An approach for fine-tuning pre-trained language models on domain-specific datasets."", ""A technique used to randomly generate augmented data for machine learning model training."", ""A system designed to solely retrieve relevant documents without generating new text.""], ""label"": [""A"", ""B"", ""C"", ""D""]}",A
lecture2_12,Which of the following statements accurately describes the core process of Retrieval Augmented Generation (RAG) as presented?,"{""text"": [""RAG uses an LLM to generate a query, retrieves relevant information from a database, and integrates this information into the LLM's input to produce more contextually relevant text."", ""RAG focuses on improving an LLM's internal knowledge base through continuous learning from user interactions."", ""RAG's main function is to pre-train LLMs using vast amounts of unstructured data to enhance their initial capabilities."", ""RAG primarily relies on symbolic reasoning and rule-based systems to answer complex queries without involving deep learning models.""], ""label"": [""A"", ""B"", ""C"", ""D""]}",A
lecture2_13,What is a primary advantage of employing Retrieval Augmented Generation (RAG) in AI systems?,"{""text"": [""It enhances response accuracy and timeliness by incorporating external, current data sources."", ""It completely removes the necessity for any underlying Large Language Model."", ""It mandates extensive and continuous fine-tuning of the LLM for diverse knowledge domains."", ""It ensures responses are solely based on the LLM's static training data.""], ""label"": [""A"", ""B"", ""C"", ""D""]}",A
lecture2_14,What resource is provided on this slide for a demonstration?,"{""text"": [""A Google Colab notebook link."", ""A link to a YouTube video."", ""A downloadable PDF document."", ""Instructions for a live coding session.""], ""label"": [""A"", ""B"", ""C"", ""D""]}",A
lecture2_15,"According to the slide, what is the task for the 'Pre Exercise'?","{""text"": [""Modify the current code to use sentence transformers instead of Google Gemini."", ""Modify the current code to use Google Gemini instead of sentence transformers."", ""Review and optimize the existing sentence transformer implementation."", ""Integrate Google Gemini into a new part of the application.""], ""label"": [""A"", ""B"", ""C"", ""D""]}",A
lecture2_16,"According to the Exercise 1 description, which Large Language Model (LLM) is specified for use in the Telegram bot?","{""text"": [""Google Gemini"", ""OpenAI GPT-4"", ""Llama 2"", ""Azure OpenAI Service""], ""label"": [""A"", ""B"", ""C"", ""D""]}",A
lecture2_17,"According to Optional Exercise 2, what specific modification should be implemented for the Telegram Bot's interaction with the LLM API?","{""text"": [""Process every three user questions into a single call to the LLM API."", ""Ensure each user question results in an individual, separate call to the LLM API."", ""Integrate a completely new LLM API provider to improve response accuracy."", ""Modify the RAG component to retrieve more documents for each user query.""], ""label"": [""A"", ""B"", ""C"", ""D""]}",A
lecture2_18,What is the primary function of the links provided under 'Resources'?,"{""text"": [""To offer supplementary materials and external articles for in-depth learning on topics like RAG and chunking."", ""To list key takeaways and summaries from the lecture presentation."", ""To acknowledge the original sources of direct quotes or data presented in the lecture."", ""To suggest AI-powered applications and tools for practical implementation.""], ""label"": [""A"", ""B"", ""C"", ""D""]}",A
lecture3_1,Which of the following describes the difference between an API and a Library?,"{""text"": [""A library is a collection of pre-written code that you can import and use directly in your program, while an API is a set of rules and protocols that allow different software applications to communicate with each other."", ""An API is a collection of pre-written code that you can import and use directly in your program, while a library is a set of rules and protocols that allow different software applications to communicate with each other."", ""There is no significant difference; they are interchangeable terms."", ""An API is only used for web services, whereas a library is used for local program functions.""], ""label"": [""A"", ""B"", ""C"", ""D""]}",A
lecture3_2,"Based on the provided slide content, what is a key highlight about Cursor's performance?","{""text"": [""Cursor grew its Annual Recurring Revenue (ARR) from 1 million to 100 million within 12 months."", ""Cursor is capable of writing 1 million lines of code per day."", ""Cursor's ARR growth was noted as one of the slowest in its industry over the past year."", ""Cursor achieved 100 million ARR after 100 months of operation.""], ""label"": [""A"", ""B"", ""C"", ""D""]}",A
lecture3_3,"What is the primary topic of this lecture, according to the slide title?","{""text"": [""Introduction to LLM Applications and Cost Optimization"", ""The history of Innopolis University"", ""Biography of Hamza Salem"", ""Advanced machine learning algorithms""], ""label"": [""A"", ""B"", ""C"", ""D""]}",A
lecture3_4,"According to the 'Who am I?' slide, which statement accurately describes Hamza Salem's public profile?","{""text"": [""He is a Tech YouTuber with over 80,000 subscribers."", ""He is primarily known as a financial consultant for major corporations."", ""He holds a Master's degree and works as a freelance web developer."", ""He specializes exclusively in traditional database systems and education.""], ""label"": [""A"", ""B"", ""C"", ""D""]}",A
lecture3_5,"According to the course goal outlined in the slide, what is the primary objective students are expected to achieve?","{""text"": [""To create an optimized architecture that involves LLM as a main or supportive component for web or mobile applications."", ""To become proficient in navigating and utilizing the Moodle platform for course materials and submissions."", ""To develop new Large Language Models (LLMs) from scratch for various real-world problems."", ""To explore and understand the fundamental differences between web and mobile application development frameworks.""], ""label"": [""A"", ""B"", ""C"", ""D""]}",A
lecture3_6,"According to the grading policy, what is the minimum overall percentage required to earn an 'A' grade?","{""text"": [""85%"", ""65%"", ""50%"", ""30%""], ""label"": [""A"", ""B"", ""C"", ""D""]}",A
lecture3_7,"According to the slide, what two key professional outcomes or paths are highlighted for individuals working with LLMs/AI?","{""text"": [""Publishing Q1/Q2 research papers and establishing a company based on one's creation."", ""Contributing to popular open-source LLM projects and securing a patent for an invention."", ""Pursuing advanced academic degrees and becoming a senior researcher in a large corporation."", ""Attending industry-leading AI conferences and consulting for multiple tech firms.""], ""label"": [""A"", ""B"", ""C"", ""D""]}",A
lecture3_8,"According to the provided lecture schedule, when is ""Efficient Inference (RAG, Prompt engineering)"" covered?","{""text"": [""Week 3-4"", ""Week 1"", ""Week 2"", ""Week 5""], ""label"": [""A"", ""B"", ""C"", ""D""]}",A
lecture3_10,Which of the following topics is NOT explicitly mentioned in the lecture agenda?,"{""text"": [""Advanced Prompt Engineering Techniques"", ""Cost Optimization in LLM Deployments"", ""Real-World Applications of LLMs"", ""Introduction to Large Language Models (LLMs)""], ""label"": [""A"", ""B"", ""C"", ""D""]}",A
lecture3_11,Which statement best describes Large Language Models (LLMs) according to the provided text?,"{""text"": [""Advanced AI systems trained on vast amounts of text data to understand, generate, and manipulate human language."", ""AI systems primarily designed for image recognition and processing."", ""Small-scale models with limited parameters focused on specific, narrow tasks."", ""Robotic systems capable of physical interaction and navigation in complex environments.""], ""label"": [""A"", ""B"", ""C"", ""D""]}",A
lecture3_12,"According to the provided content, which of the following is a real-world application of Large Language Models (LLMs)?","{""text"": [""Automated article, blog post, and social media content generation."", ""Directly managing physical supply chain operations."", ""Operating heavy machinery in construction sites."", ""Designing complex architectural blueprints for buildings.""], ""label"": [""A"", ""B"", ""C"", ""D""]}",A
lecture3_13,"According to the provided content, what is the primary purpose of the 'Preprocessing' step in how LLMs work?","{""text"": [""Adjusting the model for specific tasks after initial training."", ""Generating outputs based on input prompts."", ""Cleaning and preparing massive datasets for training."", ""Using neural networks to learn patterns from the data.""], ""label"": [""A"", ""B"", ""C"", ""D""]}",C
lecture3_14,"According to the provided content, what is the primary function of the ""Text embedding"" stage in how LLMs work?","{""text"": [""The LLM uses its 'brain' (neural network) to process the input and solve a puzzle."", ""The LLM converts input tokens into vectors, placing each token at a specific point in a high-dimensional space."", ""The LLM uses the 'transformer' model to understand how all the input words link together."", ""The LLM changes the internal number codes back into human-readable words for the final output.""], ""label"": [""A"", ""B"", ""C"", ""D""]}",B
lecture3_15,"According to the provided summary of how LLMs work, what is the primary function of the ""Embedding"" step?","{""text"": [""To break down the input text into individual words or subwords."", ""To convert the processed vectors back into human-readable text."", ""To represent each token as a numerical vector for processing."", ""To use self-attention to focus on key words.""], ""label"": [""A"", ""B"", ""C"", ""D""]}",C
lecture3_16,"According to the lecture slide, what key activities are encompassed in the 'Monitoring and Maintenance' phase when productionizing LLM applications?","{""text"": [""Initial testing and validation of the application."", ""Choosing the right model based on the specific task and performance requirements."", ""Ongoing updates, error tracking, and model performance checks."", ""Scaling the solution for real-world use.""], ""label"": [""A"", ""B"", ""C"", ""D""]}",C
lecture3_17,"According to the lecture slide on the ""Importance of Cost Optimization,"" which of the following is presented as a key strategy for managing costs in LLM operations?","{""text"": [""Use smaller, task-specific models when possible."", ""Prioritize the development and deployment of larger, more generalized models."", ""Increase reliance on real-time, individual request processing to ensure immediate responses."", ""Focus exclusively on acquiring the most expensive, cutting-edge GPU hardware for all operations.""], ""label"": [""A"", ""B"", ""C"", ""D""]}",A
lecture3_18,"In the provided example of cost optimization for an LLM-powered customer support chatbot, which model selection strategy was implemented to manage costs?","{""text"": [""Fine-tuning a smaller LLM instead of using the largest available model."", ""Deploying the largest available LLM to ensure comprehensive responses."", ""Developing a new, custom LLM from scratch for specific customer needs."", ""Using multiple large LLMs in parallel to handle high query volumes.""], ""label"": [""A"", ""B"", ""C"", ""D""]}",A
lecture3_19,"According to the lecture slide, what is a key capability enabled by the Transformer architecture and its attention mechanism in modern LLMs?","{""text"": [""Efficiently handling longer text sequences."", ""Reducing the total number of parameters in the model."", ""Eliminating the need for any pre-training phase."", ""Processing only short, fixed-length input sequences.""], ""label"": [""A"", ""B"", ""C"", ""D""]}",A
lecture3_20,"According to the key takeaways, what is a critical factor for making Large Language Models (LLMs) viable in production environments?","{""text"": [""Cost optimization."", ""Expanding their application scope."", ""Maximizing the number of LLMs deployed."", ""Minimizing the understanding of LLM mechanisms.""], ""label"": [""A"", ""B"", ""C"", ""D""]}",A
lecture3_21,"Based on the concept implied by 'Context Engineering' in the context of AI, what is its primary focus?","{""text"": [""Designing and optimizing the surrounding information, data, and environmental factors that influence an AI model's understanding and performance."", ""Developing new algorithms for real-time robotic navigation and obstacle avoidance."", ""Engineering advanced hardware components and chip designs specifically for AI processing units."", ""Creating innovative user interfaces and experiences for AI-powered applications.""], ""label"": [""A"", ""B"", ""C"", ""D""]}",A
lecture3_22,"According to the lecture slide, what is the primary goal of Context Engineering?","{""text"": [""To guide an LLM's reasoning and output by providing relevant information without altering its core weights."", ""To fundamentally change an LLM's internal knowledge base through fine-tuning."", ""To enable an LLM to \""know\"" new information and develop its own reasoning abilities."", ""To predict the most likely next token based solely on the LLM's pre-trained parameters.""], ""label"": [""A"", ""B"", ""C"", ""D""]}",A
lecture3_23,"According to the provided slide content, which of the following is a key benefit of Context Engineering?","{""text"": [""It reduces hallucinations by grounding the model in facts."", ""It primarily focuses on increasing the model's training speed."", ""It replaces the need for any form of model pre-training."", ""It ensures the output is always completely original and unreferenced.""], ""label"": [""A"", ""B"", ""C"", ""D""]}",A
lecture3_24,"According to the slide, which statement best describes an LLM's ""context window"" and a key consideration regarding it?","{""text"": [""It's the total amount of text (input + output) an LLM can process in a single interaction and is a finite, limited resource."", ""It's an unlimited memory space used for storing all past interactions, making API calls cheaper."", ""It primarily stores only the LLM's generated output, and its size is always constant across all models."", ""It's a virtual canvas for creating images, and its primary goal is to reduce computation time.""], ""label"": [""A"", ""B"", ""C"", ""D""]}",A
lecture3_25,"According to the slide, what is Cursor?","{""text"": [""An IDE built for programming that deeply integrates an LLM."", ""A new type of LLM specifically designed for code generation."", ""A framework for basic context engineering in software development."", ""A tool for analyzing lecture slides and extracting key information.""], ""label"": [""A"", ""B"", ""C"", ""D""]}",A
lecture3_26,"According to the lecture slide, what is the primary purpose of Cursor providing the LLM with 'The Global Context (The Project)'?","{""text"": [""To give the LLM the \""big picture\"" so it can generate code that is consistent with your project's structure, dependencies, and conventions."", ""To allow the LLM to perform precise, localized tasks like fixing a bug or writing a function within a specific file."", ""To include the specific file being edited, selected code, and error messages for immediate task execution."", ""To provide the LLM with real-time updates from the terminal and user-written comments for code generation.""], ""label"": [""A"", ""B"", ""C"", ""D""]}",A
lecture3_27,"What is the primary function of Cursor's ""Interaction Context,"" which maintains a chat history within the editor?","{""text"": [""To enable iterative code refinement by maintaining a coherent conversation where the AI understands references."", ""To strictly limit the AI's responses to only the current line of code being edited."", ""To document all programming language syntax errors detected during development."", ""To generate a summary of all project files edited in the last session.""], ""label"": [""A"", ""B"", ""C"", ""D""]}",A
lecture3_28,"According to the slide, when a user adds a new feature, what are the primary types of context Cursor provides to assist them?","{""text"": [""Local, Global, and Instructional context."", ""User Action, File Structure, and Project Dependencies."", ""Semantic, Syntactic, and Algorithmic context."", ""Historical, Real-time, and Predictive context.""], ""label"": [""A"", ""B"", ""C"", ""D""]}",A
lecture3_29,"According to the slide, Cursor's Context Engineering achieves highly specific and useful output by:","{""text"": [""Seamlessly weaving together project rules, local code, and user intent."", ""Solely focusing on robust regex patterns for data validation tasks."", ""Prioritizing universally applicable code over a project's unique style and conventions."", ""Generating JSDoc comments only when explicitly requested by the user.""], ""label"": [""A"", ""B"", ""C"", ""D""]}",A
lecture3_30,"According to the lecture slide, which of the following best describes the key principles for achieving successful outcomes with Large Language Models (LLMs)?","{""text"": [""Providing clear, structured context including role, goal, constraints, and examples, as the LLM's output directly reflects its input."", ""Utilizing the most advanced LLM models exclusively, as model architecture is the primary determinant of output quality."", ""Minimizing input context to allow the LLM more creative freedom, as too much structure can hinder its performance."", ""Solely relying on specialized tools like Cursor to automate context delivery, as manual context provision is inherently inefficient.""], ""label"": [""A"", ""B"", ""C"", ""D""]}",A
lecture3_31,"According to the Lab 01 instructions, what information must the Telegram bot ask the user for *before* replying to any questions?","{""text"": [""Weight, height, and age."", ""The user's preferred LLM (e.g., OpenAI, Google Gemini)."", ""Current diet and calisthenics routine."", ""Preferred language and geographic location.""], ""label"": [""A"", ""B"", ""C"", ""D""]}",A
lecture3_32,What is the primary function of the content presented on this lecture slide?,"{""text"": [""To provide supplementary learning materials and external references."", ""To summarize the key points or conclusions of the lecture."", ""To list the technical terms and their definitions used in the lecture."", ""To detail the contact information of the lecturer or teaching assistants.""], ""label"": [""A"", ""B"", ""C"", ""D""]}",A
lecture4_1,"What is the primary topic of Lecture 04, according to the provided slide content?","{""text"": [""Model Selection and Alternative Approaches"", ""The history of Innopolis University"", ""The biography of Hamza Salem"", ""An introduction to neural networks""], ""label"": [""A"", ""B"", ""C"", ""D""]}",A
lecture4_2,"Based on the provided agenda, which of the following best describes the primary topics to be discussed in this lecture?","{""text"": [""Criteria for selecting LLMs, the role of compact models, and effective prompting strategies."", ""In-depth technical analysis of LLM fine-tuning and pre-training methodologies."", ""The historical evolution of natural language processing and its foundational theories."", ""Ethical considerations and bias mitigation techniques in advanced AI systems.""], ""label"": [""A"", ""B"", ""C"", ""D""]}",A
lecture4_3,"According to the lecture slide, which of the following are key factors to consider when selecting the right model?","{""text"": [""Purpose, performance metrics, and available computational resources."", ""The model's core programming language, its development team size, and the project's overall timeline."", ""The default learning rate, the number of training epochs, and the type of optimizer used during training."", ""Social media popularity, brand recognition, and the current industry trends.""], ""label"": [""A"", ""B"", ""C"", ""D""]}",A
lecture4_4,"According to the 'Comparative Case Study' slide, which statement accurately distinguishes Model A (large, general-purpose) from Model B (smaller, task-specific)?","{""text"": [""Model B typically requires fewer resources and is cheaper to deploy and run than Model A, though Model A might be more accurate in general."", ""Model A is significantly cheaper to deploy and uses fewer computational resources than Model B."", ""Model B is generally more accurate than Model A across all tasks due to its specialized nature."", ""Both Model A and Model B have similar resource requirements and deployment costs, differing primarily in their accuracy profiles.""], ""label"": [""A"", ""B"", ""C"", ""D""]}",A
lecture4_5,"According to the lecture slide, which of the following is a key benefit of using smaller models?","{""text"": [""Faster inference times and lower computational requirements."", ""Increased model complexity for advanced reasoning."", ""Higher initial development costs due to specialized training."", ""Reduced need for fine-tuning and adaptation to specific tasks.""], ""label"": [""A"", ""B"", ""C"", ""D""]}",A
lecture4_6,"Which of the following BERT variants is specifically designed for mobile and edge applications, prioritizing a balance between performance and efficiency?","{""text"": [""TinyBERT"", ""DistilBERT"", ""ALBERT"", ""Standard BERT""], ""label"": [""A"", ""B"", ""C"", ""D""]}",A
lecture4_7,"According to the provided content, which of the following is a described advantage of leveraging general models for specialized tasks through prompt engineering?","{""text"": [""General models can be adapted to various tasks without needing fine-tuning."", ""It primarily requires extensive fine-tuning for each new specialized task."", ""It exclusively focuses on developing new specialized model architectures."", ""It guarantees superior performance over all purpose-built specialized models.""], ""label"": [""A"", ""B"", ""C"", ""D""]}",A
lecture4_8,What is the primary content presented on this slide?,"{""text"": [""A link to a practical demonstration on Google Colab."", ""A summary of key lecture concepts."", ""A list of recommended readings for the topic."", ""A set of multiple-choice practice questions.""], ""label"": [""A"", ""B"", ""C"", ""D""]}",A
lecture4_9,"According to the lecture slide's 'Key Takeaways,' which of the following statements is INCORRECT?","{""text"": [""Model selection should take into account the task, performance needs, resource constraints, and deployment environment."", ""Smaller, compact models are often more cost-efficient and effective for general, unspecialized tasks."", ""Prompt engineering allows for optimizing general models for various tasks, thus reducing the need for multiple specialized models."", ""Compact models are highlighted as being highly effective and cost-efficient for specific tasks.""], ""label"": [""A"", ""B"", ""C"", ""D""]}",B
lecture4_10,"According to Exercise 1, which two distinct components are explicitly required for the Telegram bot to generate its two-part answer?","{""text"": [""An LLM API (like Gemini) and distilbert."", ""A vector database and a web scraping library."", ""A relational database and a sentiment analysis model."", ""OpenCV and a speech-to-text API.""], ""label"": [""A"", ""B"", ""C"", ""D""]}",A
lecture4_11,"Based on the 'Exercise 2' prompts, what is the central theme these questions encourage the user to explore and implement in their bot?","{""text"": [""Optimizing the number of tokens and model usage through effective document chunking strategies."", ""Improving the bot's ability to handle complex, multi-turn conversations and maintain context."", ""Enhancing the security protocols and data privacy measures for information stored in the vector database."", ""Developing advanced user interfaces for more intuitive interaction with the bot.""], ""label"": [""A"", ""B"", ""C"", ""D""]}",A
lecture4_12,What is the main function of the provided lecture slide content?,"{""text"": [""To offer external links for additional exploration of the TinyBERT topic."", ""To present a detailed overview of the TinyBERT model's architecture."", ""To list the authors and publication dates of the TinyBERT research."", ""To summarize the main conclusions drawn from the TinyBERT research.""], ""label"": [""A"", ""B"", ""C"", ""D""]}",A
lecture5_1,"Based on this lecture slide, what are the main topics that will be discussed?","{""text"": [""Infrastructure and Deployment, Tuning Strategies, and Chunking Methods."", ""Innopolis university's research, Hamza Salem's career, and advanced AI."", ""Software architecture, system integration, and data management techniques."", ""Cloud services, network security, and application development workflows.""], ""label"": [""A"", ""B"", ""C"", ""D""]}",A
lecture5_2,"Based on the provided agenda, which of the following topics will NOT be covered in this lecture?","{""text"": [""Key strategies for optimal model deployment"", ""Inference Acceleration Tools"", ""Model Training Techniques"", ""Monitoring and Observability""], ""label"": [""A"", ""B"", ""C"", ""D""]}",C
lecture5_3,"According to the slide, what is the primary purpose of fine-tuning large language models (LLMs)?","{""text"": [""To adapt them for specific tasks and improve their performance."", ""To reduce their computational training time."", ""To convert them into smaller, non-language models."", ""To decrease the number of parameters in the model.""], ""label"": [""A"", ""B"", ""C"", ""D""]}",A
lecture5_4,"Which tuning strategy, as described in the slide, involves training a model on datasets that include pairs of instructions and expected outputs to improve its ability to follow human instructions effectively?","{""text"": [""Instruction Fine-Tuning"", ""Full Fine-Tuning"", ""Transfer Learning"", ""Parameter-Efficient Fine-Tuning""], ""label"": [""A"", ""B"", ""C"", ""D""]}",A
lecture5_5,"According to the best practices for fine-tuning, which of the following is crucial for optimizing model performance and preventing issues like overfitting or underfitting?","{""text"": [""Using an extremely small and irrelevant dataset."", ""Avoiding any form of hyperparameter adjustments."", ""Regularly evaluating the model's performance only after all training is complete."", ""Experimenting with different settings for learning rates, batch sizes, and training epochs.""], ""label"": [""A"", ""B"", ""C"", ""D""]}",D
lecture5_6,"According to the lecture slide, which strategy for optimal LLM deployment involves implementing a series of models, each trained on a different task, to optimize costs?","{""text"": [""Cascading models"", ""Mixtural models"", ""Setting probability thresholds for model output"", ""Utilizing model quantization techniques""], ""label"": [""A"", ""B"", ""C"", ""D""]}",A
lecture5_7,"According to the 'Input Management' section of the key strategies for optimal deployment, which of the following best describes recommended practices?","{""text"": [""Strategically truncate input sequences to reduce memory usage and utilize cost-efficient models like a Router for task routing."", ""Increase the length of input sequences to maximize context and always use the most powerful, regardless of cost, for all tasks."", ""Prioritize only memory usage by avoiding any form of input truncation and never considering specialized routing models."", ""Allow unrestricted input length to capture all possible data and exclusively use models that demand more tokens for better performance.""], ""label"": [""A"", ""B"", ""C"", ""D""]}",A
lecture5_8,"According to the lecture slide on 'Key strategies for optimal deployment,' which of the following is a method for memory optimization?","{""text"": [""Model Pruning"", ""Data Augmentation"", ""Increasing model complexity"", ""Early Stopping""], ""label"": [""A"", ""B"", ""C"", ""D""]}",A
lecture5_9,"According to the lecture slide, what is the primary advantage or purpose of TPUs (Tensor Processing Units) compared to CPUs and GPUs?","{""text"": [""They are Google’s custom-made powerful processors built specifically to train and learn machine learning models faster."", ""They serve as the brain of any computer system, responsible for managing logical, arithmetic, and I/O operations."", ""They are additional processors capable of performing advanced graphics rendering and complex mathematical operations."", ""They are primarily chosen for cloud-based deployments due to their inherent security features for sensitive data.""], ""label"": [""A"", ""B"", ""C"", ""D""]}",A
lecture5_10,"According to the ""Continuous Monitoring and Optimization"" strategy for optimal LLM deployment, which of the following best describes its core components?","{""text"": [""Continuously experimenting to balance cost and performance, staying updated on new LLM models and optimization techniques, and prioritizing ethical considerations like data privacy."", ""Primarily focusing on achieving the lowest possible operational cost, even if it compromises performance or ethical standards."", ""Implementing a \""deploy once, optimize never\"" strategy to minimize ongoing management efforts after initial setup."", ""Exclusively adopting the most expensive, state-of-the-art LLM models to ensure maximum performance, irrespective of cost-effectiveness or ethical implications.""], ""label"": [""A"", ""B"", ""C"", ""D""]}",A
lecture5_11,"According to the provided information, what is Medusa's primary mechanism for accelerating Large Language Model (LLM) inference?","{""text"": [""It adds extra decoding heads to predict multiple subsequent tokens in parallel."", ""It utilizes a separate draft model to generate candidate tokens for verification by the main LLM."", ""It quantizes the model weights to a lower precision, reducing computational requirements."", ""It prunes redundant layers and connections within the LLM architecture to create a smaller model.""], ""label"": [""A"", ""B"", ""C"", ""D""]}",A
lecture5_13,What mechanism does the Lookahead framework use to guarantee correct output without approximation?,"{""text"": [""Trie-based retrieval and verification."", ""A probabilistic sampling method."", ""Early exit conditions based on confidence scores."", ""Batch processing of individual token predictions.""], ""label"": [""A"", ""B"", ""C"", ""D""]}",A
lecture5_14,"According to the 'Three Pillars of Observability' for LLM monitoring, which component is primarily used to track execution paths through various system components, aiding in identifying the causes of performance issues and understanding interactions?","{""text"": [""Traces"", ""Logs"", ""Metrics"", ""Alerts""], ""label"": [""A"", ""B"", ""C"", ""D""]}",A
lecture5_15,"According to the provided content, which of the following is a benefit of LLM Observability?","{""text"": [""Faster Issue Diagnosis"", ""Automated model version control"", ""Reduced need for data preprocessing"", ""Enhanced model training speed""], ""label"": [""A"", ""B"", ""C"", ""D""]}",A
lecture5_16,What is the primary purpose of 'Chunking/Text Splitters' in natural language processing (NLP) applications?,"{""text"": [""To break down large documents into smaller, manageable pieces for processing by models with limited input context windows."", ""To identify and correct grammatical errors and typos within a text."", ""To convert text into different character encodings or binary formats."", ""To extract the main topics or sentiment of a document.""], ""label"": [""A"", ""B"", ""C"", ""D""]}",A
lecture5_17,"According to the provided slide, what are the main reasons for performing 'chunking' when preparing text for Large Language Models (LLMs)?","{""text"": [""To reduce token usage and provide the LLM with accurate and manageable input prompts."", ""To combine multiple small documents into a single, larger input for the LLM to process."", ""To enhance the LLM's creativity and ability to generate diverse output."", ""To convert the text into a different language for multilingual LLMs.""], ""label"": [""A"", ""B"", ""C"", ""D""]}",A
lecture5_18,"According to the 'Chunking theories/Methods' slide, what are the primary considerations discussed when determining how to split content and the characteristics of the resulting chunks?","{""text"": [""How to split the content (e.g., by characters) and whether the chunk size should be fixed or dynamic."", ""The specific programming language used for chunking and the availability of pre-trained chunking models."", ""The licensing of the content and the target audience's reading comprehension level."", ""Only the fixed number of characters per chunk, without considering dynamic sizing or other splitting methods.""], ""label"": [""A"", ""B"", ""C"", ""D""]}",A
lecture5_19,"According to the lecture slide, which of the following is NOT presented as a method for chunking content?","{""text"": [""By paragraph or by sentence for a book."", ""By code/tag or by context for a web page."", ""By message or by time for a conversion/chat."", ""By author or by publication date for a document.""], ""label"": [""A"", ""B"", ""C"", ""D""]}",D
lecture5_20,"Based on the lecture slide, which of the following is *not* listed as a 'must-have' component for your project?","{""text"": [""Retrieval Augmented Generation (RAG)"", ""Chunking"", ""Memory"", ""A vector database (as part of RAG)""], ""label"": [""A"", ""B"", ""C"", ""D""]}",C
lecture5_21,"According to the exercise, what are the required inputs for the `/answer` endpoint?","{""text"": [""A string representing a question and an array of context chunks."", ""A string of text."", ""An array of strings, where each string represents a context chunk."", ""A string representing a question.""], ""label"": [""A"", ""B"", ""C"", ""D""]}",A
lecture5_22,"Based on the provided exercise, what is the complete functionality of the API endpoint to be created?","{""text"": [""It performs semantic text chunking based on contextual similarity, analyzes the resulting chunks, groups them where necessary, and suggests a similarity threshold for each group based on gaps between adjacent chunks."", ""It splits text into fixed-size chunks, identifies duplicate chunks for removal, and provides a single global similarity score for the entire document."", ""It generates new text summaries by combining semantically similar chunks and determines an optimal length for the generated summaries."", ""It visualizes the similarity relationships between all words in a document, allowing users to manually define chunk boundaries and set their own similarity thresholds.""], ""label"": [""A"", ""B"", ""C"", ""D""]}",A
lecture5_23,What is the primary purpose of the content presented on this slide?,"{""text"": [""To provide external links and references for additional study."", ""To summarize the main concepts covered in the lecture."", ""To introduce new topics that will be discussed next."", ""To list the authors and contributors to the lecture material.""], ""label"": [""A"", ""B"", ""C"", ""D""]}",A
